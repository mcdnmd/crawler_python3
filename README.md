# crawler 

Автор: Полтораднев Кирилл

## Описание
Утилита краулер позволяет скачивать контент с выбранного сайта. Зная глубину и стартовую ссылку, робот начинает скачивать содержимое сайта в пределах глубины. Все ссылки сохраняются, соглсасно изначальной кодировке, в выбранной папке или в корневой папке с рандомным именем.


## Состав
* Файл запуска утилиты: `startup.py`
* Модули: `/modules`
* Тесты: `/tests`

### Управление
`python3 startup.py [url] -f [folder] -j [max_threads] -d [depth] -F [filters] [filtred_size]`

Для работы утилита использует параметры:

* URL или ip-адресс  веб-сайта `url`.
* Директория для выгрузки страницы `-f <folder>`.
* Максимально разрешенное количество потоков `-j <max_threads>`, по умолчанию 2 * кол-во ядер.
* Глубина поиска `-d <depth>`, по умолчанию 5.
* Фильтры для расширений файлов в ссылках и размера `-F <extension_filter> <size>`

## Подробности реализации
Модули, отвечающие за логику краулера расположены в пакете modules. 
* `modules.TerminalParser` - класс, обрабатывающий данные командной строки. Проверяет ввод на соответсвие стандартам.
* `modules.Crawler` - класс, реализующий основную логику краулинга. Разгребает очередь из url-ов, формирует набор потоков, запускает выполнение потоков.
* `modules.RobotsHandler` - класс, реализующий запрос и обработку правил из *robots.txt.
* `modules.PageParser`- класс, реализующий обрабоку страницы. Возвращает массив из строк - ещё не посещенный url-ов.
* `modules.LinkParser`- класс-потомок `html.parse.HTMLParser`, реализующий обработку тегов из HTML файла.
* `modules.FileSystemHandler` - класс, реализующий методы взаимодействия с файловой системой компьютера.
* `modules.HTTPClient` - серви отправки HTTP запросов и первичной обработке заголовков.
* `modules.Url` - класс-структура, описывающая сущность URL

### Дополнительно
В программе используется автодокументирование Doxygen
