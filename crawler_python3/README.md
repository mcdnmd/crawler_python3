# crawler 
Version: 0.2

Автор: Полтораднев Кирилл

## Описание
Утилита краулер позволяет скачивать контент с выбранного сайта.Зная глубину погружения и стартовую ссылку, робот начинает скачивать все ссылки в пределах глубины. Все ссылки сохраняются, соглсасно изначальной кодировке, в выбранной папке.


## Состав
* Консольная вресия: `startup.py`
* Модули: `/modules`
* Тесты: `/tests`

### Управление
`python3 startup.py [web_url] -f [folder] -s [chunk_size] -d [depth] -ef [simple_filter]`

Для работы утилиты нужно задать параметры

* Url веб-сайта `web-url`
* Директорию, куда выгрузим страницы `-f <folder>`
* Размер чанка для выгрузки страницы `-s <chunk_size>`, по дефолту стоит 512
* Глубину поиска `-d <depth>`, по дефолту стоит 5
* Фильтры на расширение в ссылках `-ef <simple_filter>`

## Подробности реализации
Модули, отвечающие за логику краулера расположены в пакете modules. В основе лежaт: 
* `modules.Crawler` - класс, формирующий цикл обработки данных
* `modules.PageParser`- класс, реализующий обрабоку страницы и верификацию ссылок
* `modules.LinkParser`- класс-потомок html.parse.HTMLParser, реализующий обработку данных со страницы HTML
